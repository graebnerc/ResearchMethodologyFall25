{
  "hash": "fb95ad4c2cbf91a7ffa2402896beee13",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Implementation Lab: Linear Regression & Experimental Analysis\"\nsubtitle: \"Complete Solutions\"\nformat:\n  html:\n    toc: true\n    code-fold: show\n    theme: cosmo\n---\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(effectsize)\nlibrary(car)\nlibrary(gridExtra)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the datasets\nmarketing_data <- read.csv(\"marketing_data.csv\")\nfirm_growth_data <- read.csv(\"firm_growth_data.csv\")\nleadership_study_between <- read.csv(\"leadership_study_between.csv\")\ncommunication_study <- read.csv(\"communication_study.csv\")\nexercise_data <- read.csv(\"exercise_data.csv\")\n```\n:::\n\n\n---\n\n# Part 1: Linear Regression\n\n## 1.1 Simple vs. Multiple Regression\n\n**Research Question:** How does ad spending affect sales revenue?\n\n### Simple Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit simple regression: sales_revenue ~ ad_spend\nmodel_simple <- lm(sales_revenue ~ ad_spend, data = marketing_data)\nsummary(model_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sales_revenue ~ ad_spend, data = marketing_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.000 -14.155  -1.484  10.909  48.889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 67.52803    4.06035   16.63   <2e-16 ***\nad_spend     3.03673    0.06417   47.32   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.35 on 98 degrees of freedom\nMultiple R-squared:  0.9581,\tAdjusted R-squared:  0.9576 \nF-statistic:  2239 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n**Coefficient interpretation:**\n\nFor every €1 increase in ad spending, sales revenue increases by approximately €3.04.\n\nThis is the **total association** between ad spending and sales - it captures both the direct effect of advertising and any indirect effects through other variables.\n\n### Multiple Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add website_traffic to the model\nmodel_multiple <- lm(sales_revenue ~ ad_spend + website_traffic, data = marketing_data)\nsummary(model_multiple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = sales_revenue ~ ad_spend + website_traffic, data = marketing_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-31.4339  -9.9212  -0.4957   9.7412  31.2228 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      15.7527     7.9513   1.981   0.0504 .  \nad_spend          2.1017     0.1407  14.940  < 2e-16 ***\nwebsite_traffic   1.1021     0.1540   7.158 1.58e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.11 on 97 degrees of freedom\nMultiple R-squared:  0.9726,\tAdjusted R-squared:  0.972 \nF-statistic:  1719 on 2 and 97 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n**Coefficient interpretation:**\n\nNow the coefficients represent **ceteris paribus effects** (direct effects, controlling for other variables):\n\n- For every €1 increase in ad spending, holding website traffic constant, sales revenue increases by approximately €2.1.\n- For every additional visitor, holding ad spending constant, sales revenue increases by approximately €1.1.\n\n### Compare Coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract coefficient for ad_spend from both models\ncoef_simple <- coef(model_simple)[\"ad_spend\"]\ncoef_multiple <- coef(model_multiple)[\"ad_spend\"]\n\ncat(\"Simple model coefficient:\", round(coef_simple, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple model coefficient: 3.037 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Multiple model coefficient:\", round(coef_multiple, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMultiple model coefficient: 2.102 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Difference (bias):\", round(coef_simple - coef_multiple, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference (bias): 0.935 \n```\n\n\n:::\n:::\n\n\n**Understanding Omitted Variable Bias:**\n\nThe coefficient changed from 3.04 to 2.1 because:\n\n1. **Website traffic is correlated with ad spending** (ads drive traffic)\n2. **Website traffic affects sales** (more visitors → more sales)\n3. **The simple model attributed both effects to ad spending** (overestimated the direct effect)\n\nThis is called **omitted variable bias** - when we leave out a relevant variable that correlates with both our predictor and outcome, we get biased estimates.\n\n**Business implication:** The direct ROI of advertising (€2.1 per €1 spent) is lower than the total association (€3.04 per €1 spent) because some of advertising's impact works through increased website traffic.\n\n---\n\n## 1.2 Model Diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract fitted values and residuals using base R\nfitted_values <- fitted(model_multiple)\nresiduals_values <- residuals(model_multiple)\n\n# Create diagnostic plots\np1 <- ggplot(data.frame(fitted = fitted_values, resid = residuals_values),\n             aes(x = fitted, y = resid)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(title = \"Residuals vs Fitted\",\n       subtitle = \"Should show random scatter (no pattern)\",\n       x = \"Fitted Values\", \n       y = \"Residuals\") +\n  theme_minimal()\n\np2 <- ggplot(data.frame(resid = residuals_values), aes(sample = resid)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Normal Q-Q Plot\",\n       subtitle = \"Points should follow red line\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/diagnostic-plots-1.png){width=960}\n:::\n:::\n\n\n**What to look for:**\n\n1. **Residuals vs Fitted:** \n   - Should show random scatter around the horizontal line at 0\n   - No clear patterns (U-shapes, fan shapes, etc.)\n   - If you see a pattern, it suggests: missing variables, non-linearity, or heteroscedasticity\n\n2. **Q-Q Plot:**\n   - Points should follow the red diagonal line closely\n   - Deviations suggest non-normal residuals\n   - Small deviations at the extremes are often okay\n\n**Interpretation for our model:** Both plots look good - residuals are randomly scattered and approximately normally distributed.\n\n---\n\n## 1.3 Log Transformation\n\n**Research Question:** How has company revenue grown over time?\n\n### Visualize Raw Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create scatter plot with linear fit\nggplot(firm_growth_data, aes(x = year, y = revenue)) +\n  geom_point(size = 3, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\", linewidth = 1) +\n  labs(title = \"Revenue Growth Over Time (Raw Data)\",\n       subtitle = \"Red = Linear fit (inadequate), Blue = Flexible fit\",\n       x = \"Year\",\n       y = \"Revenue (EUR)\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/visualize-exponential-1.png){width=768}\n:::\n:::\n\n\n**Observation:**\n\nThe linear fit (red) completely fails to capture the exponential growth pattern. The data curves upward, showing that revenue growth is accelerating over time - a characteristic of exponential growth.\n\n### Apply Log Transformation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create log-transformed variable\nfirm_growth_data <- firm_growth_data %>%\n  mutate(log_revenue = log(revenue))\n\n# Fit both models\nmodel_linear <- lm(revenue ~ year, data = firm_growth_data)\nmodel_log <- lm(log_revenue ~ year, data = firm_growth_data)\n\n# Compare R²\ncat(\"Linear model R²:\", round(summary(model_linear)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear model R²: 0.9188 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Log model R²:\", round(summary(model_log)$r.squared, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLog model R²: 0.9583 \n```\n\n\n:::\n:::\n\n\nThe log model has much better fit!\n\n### Visualize Log-Transformed Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot log(revenue) vs year\nggplot(firm_growth_data, aes(x = year, y = log_revenue)) +\n  geom_point(size = 3, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", linewidth = 1) +\n  labs(title = \"Log(Revenue) vs Year\",\n       subtitle = \"Perfect linear relationship after transformation!\",\n       x = \"Year\",\n       y = \"Log(Revenue)\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/visualize-log-1.png){width=768}\n:::\n:::\n\n\nAfter log transformation, the relationship is linear! This confirms that the original data follows an exponential growth pattern.\n\n### Interpret Coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get coefficient for year in log model\ncoef_log <- coef(model_log)[\"year\"]\npercentage_change <- (exp(coef_log) - 1) * 100\n\ncat(\"Coefficient in log model:\", round(coef_log, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoefficient in log model: 0.1304 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"This means:\", round(percentage_change, 2), \"% growth per year\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis means: 13.93 % growth per year\n```\n\n\n:::\n:::\n\n\n**Key interpretation rule:**\n\nWhen the **dependent variable** is log-transformed, coefficients represent **percentage changes**.\n\nSpecifically: Each year is associated with approximately 13.9% growth in revenue.\n\n**Why this works:** Exponential relationships have the form $Y = A \\cdot e^{Bx}$. Taking the natural log gives: $\\ln(Y) = \\ln(A) + Bx$, which is linear!\n\n### Visual Proof: Back to Original Scale\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create predictions on original scale\npredictions <- firm_growth_data %>%\n  mutate(\n    linear_pred = predict(model_linear),\n    log_pred = exp(predict(model_log))  # Back-transform predictions\n  )\n\nggplot(predictions, aes(x = year)) +\n  geom_point(aes(y = revenue), size = 3, alpha = 0.8) +\n  geom_line(aes(y = linear_pred), color = \"red\", linewidth = 1.2) +\n  geom_line(aes(y = log_pred), color = \"blue\", linewidth = 1.2) +\n  labs(title = \"Model Predictions Comparison\",\n       subtitle = \"Red = Linear model (poor fit), Blue = Log model (excellent fit)\",\n       x = \"Year\",\n       y = \"Revenue (EUR)\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/back-transform-1.png){width=768}\n:::\n:::\n\n\nThe log model (blue) perfectly captures the exponential growth pattern, while the linear model (red) systematically misses the curvature.\n\n---\n\n# Part 2: Experimental Analysis\n\n## 2.1 t-Test\n\n**Research Question:** Does leadership training improve team performance?\n\n### Visualize the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(leadership_study_between, aes(x = group, y = team_performance, fill = group)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"red\") +\n  labs(title = \"Team Performance by Group\",\n       subtitle = \"Red diamonds show group means\",\n       x = \"Group\",\n       y = \"Team Performance Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/visualize-ttest-1.png){width=768}\n:::\n:::\n\n\nVisually, the training group appears to have higher performance scores on average.\n\n### Check Assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Normality test for each group\nshapiro_control <- shapiro.test(\n  filter(leadership_study_between, group == \"control\")$team_performance\n)\nshapiro_training <- shapiro.test(\n  filter(leadership_study_between, group == \"training\")$team_performance\n)\n\ncat(\"Shapiro-Wilk test - Control group: p =\", round(shapiro_control$p.value, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShapiro-Wilk test - Control group: p = 0.9312 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Shapiro-Wilk test - Training group: p =\", round(shapiro_training$p.value, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nShapiro-Wilk test - Training group: p = 0.4427 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Equal variances test\nlevene_result <- leveneTest(team_performance ~ group, data = leadership_study_between)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"\\nLevene's test for equal variances: p =\", round(levene_result$`Pr(>F)`[1], 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLevene's test for equal variances: p = 0.1078 \n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\nBoth p-values are > 0.05, so:\n- ✓ Data is approximately normally distributed in each group\n- ✓ Variances are approximately equal\n\nWe can proceed with the standard t-test.\n\n### Run t-test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_result <- t.test(\n  team_performance ~ group,\n  data = leadership_study_between,\n  var.equal = TRUE\n)\n\nprint(t_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  team_performance by group\nt = -3.7092, df = 58, p-value = 0.0004673\nalternative hypothesis: true difference in means between group control and group training is not equal to 0\n95 percent confidence interval:\n -12.154597  -3.634084\nsample estimates:\n mean in group control mean in group training \n              75.92457               83.81891 \n```\n\n\n:::\n:::\n\n\n**Results:**\n\n- **Mean difference:** 7.89 points (training group scored higher)\n- **95% CI:** [-12.15, -3.63]\n- **p-value:** 0.000467\n- **Conclusion:** The training group performed significantly better than the control group (p < 0.001)\n\n### Calculate Effect Size\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d_result <- cohens_d(team_performance ~ group, \n                            data = leadership_study_between)\nprint(cohens_d_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d |         95% CI\n--------------------------\n-0.96     | [-1.49, -0.42]\n\n- Estimated using pooled SD.\n```\n\n\n:::\n:::\n\n\n**Effect size interpretation:**\n\n- **Cohen's d = 0.96**\n- This is a **large effect** (d > 0.8)\n\n**What this means:**\n- The groups differ by more than 1 standard deviation\n- Not only is the difference statistically significant, it's also practically meaningful\n- This suggests the training has a substantial real-world impact\n\n**Business decision:** Given the large effect size and statistical significance, the leadership training appears highly effective and worth implementing if costs are reasonable.\n\n---\n\n## 2.2 One-Way ANOVA\n\n**Research Question:** Which communication method leads to highest satisfaction?\n\n### Visualize the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(communication_study, aes(x = communication_method, \n                                 y = satisfaction_score, \n                                 fill = communication_method)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, fill = \"red\") +\n  labs(title = \"Satisfaction Score by Communication Method\",\n       subtitle = \"Red diamonds show group means\",\n       x = \"Communication Method\",\n       y = \"Satisfaction Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/visualize-anova-1.png){width=768}\n:::\n:::\n\n\nThe groups appear to have different mean satisfaction scores, with email showing the lowest satisfaction.\n\n### Fit ANOVA\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Using aov()\nanova_model <- aov(satisfaction_score ~ communication_method, \n                   data = communication_study)\nsummary(anova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Df Sum Sq Mean Sq F value   Pr(>F)    \ncommunication_method  2  51.46  25.731   16.37 9.21e-07 ***\nResiduals            87 136.72   1.571                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n**Results:**\n\n- **F-statistic:** 16.37\n- **p-value:** 9.21e-07\n\n**Conclusion:** There is a statistically significant difference in satisfaction scores among the three communication methods (p < 0.001).\n\n**Important:** ANOVA only tells us that *at least one* group differs from the others. It doesn't tell us *which* groups differ - we need post-hoc tests for that.\n\n### ANOVA as Regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Show that lm() gives same results\nlm_model <- lm(satisfaction_score ~ communication_method, \n               data = communication_study)\nanova(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: satisfaction_score\n                     Df  Sum Sq Mean Sq F value    Pr(>F)    \ncommunication_method  2  51.462 25.7312  16.374 9.212e-07 ***\nResiduals            87 136.716  1.5714                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe F-statistic and p-value are identical! This demonstrates that ANOVA is just a special case of regression with categorical predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at coefficients\nsummary(lm_model)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                 Estimate Std. Error   t value     Pr(>|t|)\n(Intercept)                      5.656295  0.2288701 24.713991 4.231093e-41\ncommunication_methodface_to_face 1.641875  0.3236713  5.072663 2.195718e-06\ncommunication_methodvideo_call   1.563436  0.3236713  4.830320 5.797655e-06\n```\n\n\n:::\n:::\n\n\n**Understanding the coefficients:**\n\nR automatically creates dummy variables for categorical predictors. Since \"email\" comes first alphabetically, it becomes the **reference group**:\n\n- **Intercept (5.78):** Mean satisfaction for email group\n- **face_to_face coefficient (1.70):** Face-to-face scores 1.70 points higher than email on average\n- **video_call coefficient (1.23):** Video call scores 1.23 points higher than email on average\n\n### Effect Size\n\n\n::: {.cell}\n\n```{.r .cell-code}\neta_sq <- eta_squared(anova_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(eta_sq)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Effect Size for ANOVA\n\nParameter            | Eta2 |       95% CI\n------------------------------------------\ncommunication_method | 0.27 | [0.14, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\nη² = 0.273 means communication method explains 27.3% of the variance in satisfaction scores.\n\n**Effect size guidelines:**\n- Small: η² ≈ 0.01\n- Medium: η² ≈ 0.06\n- Large: η² ≈ 0.14\n\nOur effect (0.27) is large, indicating communication method has a substantial impact on satisfaction.\n\n### Post-Hoc Tests\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tukey HSD controls for multiple comparisons\ntukey_result <- TukeyHSD(anova_model)\nprint(tukey_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = satisfaction_score ~ communication_method, data = communication_study)\n\n$communication_method\n                               diff        lwr       upr     p adj\nface_to_face-email       1.64187523  0.8700877 2.4136627 0.0000065\nvideo_call-email         1.56343582  0.7916483 2.3352233 0.0000172\nvideo_call-face_to_face -0.07843941 -0.8502269 0.6933481 0.9681547\n```\n\n\n:::\n:::\n\n\n**Interpretation:**\n\nLooking at the p-values (p adj column):\n\n1. **face_to_face - email:** p < 0.001 → **Significant difference**\n   - Face-to-face has ~1.70 points higher satisfaction than email\n\n2. **video_call - email:** p < 0.001 → **Significant difference**\n   - Video call has ~1.23 points higher satisfaction than email\n\n3. **video_call - face_to_face:** p = 0.968 → **No significant difference**\n   - Face-to-face and video call don't differ significantly from each other\n\n**Business conclusion:** Both face-to-face and video call communication produce significantly higher satisfaction than email, but there's no meaningful difference between face-to-face and video call. Consider phasing out email-only communication for important interactions.\n\n---\n\n# Part 3: Guided Exercise - Solutions\n\n## Dataset: Website A/B Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexercise_data <- read.csv(\"exercise_data.csv\")\nhead(exercise_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  user_id design previous_visits time_on_site conversion_prob converted\n1       1 Simple              10    179.63774       0.5735567         1\n2       2 Simple              10    121.67466       0.4296563         1\n3       3 Simple               4    207.78119       0.5690097         1\n4       4 Simple               8     81.54658       0.3133435         1\n5       5 Simple               8    185.73159       0.5639764         1\n6       6 Simple               5    164.35112       0.4734029         0\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(exercise_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    user_id          design          previous_visits  time_on_site   \n Min.   :  1.00   Length:100         Min.   : 2.00   Min.   : 81.55  \n 1st Qu.: 25.75   Class :character   1st Qu.: 6.00   1st Qu.:165.38  \n Median : 50.50   Mode  :character   Median : 8.00   Median :207.41  \n Mean   : 50.50                      Mean   : 8.15   Mean   :208.77  \n 3rd Qu.: 75.25                      3rd Qu.:10.00   3rd Qu.:248.89  \n Max.   :100.00                      Max.   :18.00   Max.   :401.45  \n conversion_prob    converted   \n Min.   :0.3133   Min.   :0.00  \n 1st Qu.:0.4972   1st Qu.:0.00  \n Median :0.6035   Median :1.00  \n Mean   :0.6107   Mean   :0.59  \n 3rd Qu.:0.7359   3rd Qu.:1.00  \n Max.   :0.9141   Max.   :1.00  \n```\n\n\n:::\n:::\n\n\n---\n\n## Task 1: Regression Analysis\n\n### a. Scatter plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(exercise_data, aes(x = time_on_site, y = converted)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Conversion vs Time on Site\",\n       x = \"Time on Site (seconds)\",\n       y = \"Converted (0 = No, 1 = Yes)\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](student_solutions_files/figure-html/task1a-1.png){width=768}\n:::\n:::\n\n\nThere appears to be a positive relationship - users who spend more time on site are more likely to convert.\n\n### b. Fit simple regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_ex_simple <- lm(converted ~ time_on_site, data = exercise_data)\nsummary(model_ex_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = converted ~ time_on_site, data = exercise_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7682 -0.5514  0.3432  0.4173  0.5277 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.3969011  0.1836908   2.161   0.0332 *\ntime_on_site 0.0009249  0.0008475   1.091   0.2778  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4938 on 98 degrees of freedom\nMultiple R-squared:  0.01201,\tAdjusted R-squared:  0.001927 \nF-statistic: 1.191 on 1 and 98 DF,  p-value: 0.2778\n```\n\n\n:::\n:::\n\n\n### c. Interpret coefficient\n\n**Interpretation:**\n\nThe coefficient for `time_on_site` is 9.2\\times 10^{-4}.\n\nThis means: For every additional second spent on the site, the probability of conversion increases by approximately 9\\times 10^{-4} (or 0.09%).\n\n**Note:** This is a linear probability model. For better modeling of binary outcomes, we'd typically use logistic regression, but linear models provide a reasonable approximation for interpretation.\n\n### d. Report R²\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr2_simple <- summary(model_ex_simple)$r.squared\ncat(\"R² =\", round(r2_simple, 4), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR² = 0.012 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Time on site explains\", round(r2_simple * 100, 1), \"% of variance in conversion\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTime on site explains 1.2 % of variance in conversion\n```\n\n\n:::\n:::\n\n\nThe R² is moderate, suggesting time on site is a meaningful predictor but there are other factors influencing conversion.\n\n---\n\n## Task 2: Compare Designs\n\n### a. t-test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_test_design <- t.test(time_on_site ~ design, data = exercise_data)\nprint(t_test_design)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  time_on_site by design\nt = 7.1438, df = 89.904, p-value = 2.289e-10\nalternative hypothesis: true difference in means between group Complex and group Simple is not equal to 0\n95 percent confidence interval:\n 49.23265 87.16487\nsample estimates:\nmean in group Complex  mean in group Simple \n             242.8722              174.6735 \n```\n\n\n:::\n:::\n\n\n**Results:**\n\n- **Complex design mean:** 242.9 seconds\n- **Simple design mean:** 174.7 seconds\n- **Difference:** 68.2 seconds\n- **p-value:** 2.29e-10\n\nThe complex design keeps users on the site significantly longer (p < 0.001).\n\n### b. Effect size\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncohens_d_design <- cohens_d(time_on_site ~ design, data = exercise_data)\nprint(cohens_d_design)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCohen's d |       95% CI\n------------------------\n1.43      | [0.99, 1.87]\n\n- Estimated using pooled SD.\n```\n\n\n:::\n:::\n\n\n**Effect size:** Cohen's d = 1.43 (large effect)\n\n### c. Conclusion\n\n**Which design keeps users longer?**\n\nThe **Complex design** keeps users on the site significantly longer - about 68.2 seconds more on average. This is both statistically significant (p < 0.001) and a large practical effect (d ≈ 1.4).\n\n**Business consideration:** While complex design increases time on site, we should also check if this translates to higher conversion rates - longer time doesn't always mean better outcomes!\n\n---\n\n## Task 3: Omitted Variable Bias (Challenge)\n\n### a. Add previous_visits\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_ex_multiple <- lm(converted ~ time_on_site + previous_visits, \n                        data = exercise_data)\nsummary(model_ex_multiple)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = converted ~ time_on_site + previous_visits, data = exercise_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7242 -0.5594  0.2915  0.4246  0.5387 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)\n(Intercept)     0.2666550  0.2150695   1.240    0.218\ntime_on_site    0.0008095  0.0008518   0.950    0.344\nprevious_visits 0.0189377  0.0163406   1.159    0.249\n\nResidual standard error: 0.493 on 97 degrees of freedom\nMultiple R-squared:  0.0255,\tAdjusted R-squared:  0.005409 \nF-statistic: 1.269 on 2 and 97 DF,  p-value: 0.2857\n```\n\n\n:::\n:::\n\n\n### b. Compare coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef_simple <- coef(model_ex_simple)[\"time_on_site\"]\ncoef_multiple <- coef(model_ex_multiple)[\"time_on_site\"]\n\ncat(\"Simple model - time_on_site coefficient:\", round(coef_simple, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple model - time_on_site coefficient: 0.00092 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Multiple model - time_on_site coefficient:\", round(coef_multiple, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMultiple model - time_on_site coefficient: 0.00081 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Difference:\", round(coef_simple - coef_multiple, 5), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDifference: 0.00012 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Percentage change:\", round((coef_simple - coef_multiple) / coef_simple * 100, 1), \"%\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPercentage change: 12.5 %\n```\n\n\n:::\n:::\n\n\n### c. Interpret\n\n**Is there omitted variable bias? Why or why not?**\n\n**Yes, there is omitted variable bias!**\n\nThe coefficient for `time_on_site` changed from 9.2\\times 10^{-4} to 8.1\\times 10^{-4} when we added `previous_visits` to the model.\n\n**Why this happened:**\n\nOmitted variable bias occurs when a variable:\n1. Affects the outcome (conversion) ✓\n2. Correlates with the included predictor (time_on_site) ✓\n\nLet's check the correlation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(exercise_data$time_on_site, exercise_data$previous_visits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1169124\n```\n\n\n:::\n:::\n\n\nThe correlation is 0.117.\n\n**The story:**\n- Users with more previous visits tend to spend more time on site (they're familiar, engaged customers)\n- Users with more previous visits are also more likely to convert (trust, familiarity)\n- The simple model incorrectly attributed *some* of the \"previous visits\" effect to \"time on site\"\n\n**Conclusion:** The multiple regression model gives us a more accurate estimate of the *direct effect* of time on site, controlling for user engagement history. This is crucial for making informed design decisions!\n\n---\n\n# Summary: Key Concepts\n\n## Linear Regression\n\n1. **Simple vs. Multiple Regression:**\n   - Simple: Total association between variables\n   - Multiple: Direct effects (ceteris paribus), controlling for confounders\n\n2. **Omitted Variable Bias:**\n   - Occurs when we leave out variables that correlate with both predictor and outcome\n   - Results in biased coefficient estimates\n   - Solution: Include relevant control variables\n\n3. **Log Transformation:**\n   - Linearizes exponential relationships\n   - Coefficients represent percentage changes\n   - Check diagnostic plots to see if transformation helps\n\n4. **Model Diagnostics:**\n   - Residuals vs Fitted: Check for patterns\n   - Q-Q Plot: Check normality of residuals\n   - Use `fitted()` and `residuals()` to extract values\n\n## Experimental Analysis\n\n1. **t-Tests:**\n   - Compare means between two groups\n   - Check assumptions: normality, equal variances\n   - Always calculate effect sizes (Cohen's d)\n\n2. **ANOVA:**\n   - Compare means across 3+ groups\n   - ANOVA = regression with categorical predictors\n   - Post-hoc tests (Tukey HSD) identify which groups differ\n   - Report effect sizes (η²)\n\n3. **Effect Sizes:**\n   - Statistical significance ≠ practical significance\n   - Cohen's d guidelines: 0.2 (small), 0.5 (medium), 0.8 (large)\n   - η² guidelines: 0.01 (small), 0.06 (medium), 0.14 (large)\n\n4. **Assumptions Matter:**\n   - Always check before running tests\n   - Violations can lead to incorrect conclusions\n   - Alternatives exist when assumptions are violated\n\n---\n\n# Additional Resources\n\n## When to Use Each Method\n\n- **Simple regression:** Exploring basic relationships\n- **Multiple regression:** Controlling for confounders, real-world analysis\n- **t-test:** Comparing two groups (experiments, A/B tests)\n- **ANOVA:** Comparing 3+ groups\n- **Log transformation:** Exponential growth, multiplicative effects\n\n## Common Mistakes to Avoid\n\n1. Interpreting correlation as causation\n2. Ignoring omitted variable bias\n3. Forgetting to check assumptions\n4. Relying only on p-values without effect sizes\n5. Using multiple t-tests instead of ANOVA (inflates Type I error)\n\n## Next Steps\n\n- Review the tutorials for deeper theoretical understanding\n- Practice with your own datasets\n- Learn about logistic regression for binary outcomes\n- Explore interaction effects in factorial designs\n\n---\n\n*If you have questions about any of these concepts, please come to office hours or post in the course forum!*\n",
    "supporting": [
      "student_solutions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}